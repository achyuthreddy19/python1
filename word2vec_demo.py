# -*- coding: utf-8 -*-
"""Word2Vec Demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RgTTQbuGCGTH2CmJtV_sQGm07jDs_2t_

Sample sentences :
    
1) Welcome to Malla Reddy
2) Malla Reddy has CSE stream
3) CSE stream in Malla Reddy is best

Corpus :
Welcome, to , Malla, Reddy, hass, CSE, Stream, in, is, best  - 10 D vector

![image.png](attachment:image.png)

Training the Data for Word2Vec

![image.png](attachment:image.png)

Embeddings in ndimensional space

![image.png](attachment:image.png)
"""



What is Cosine Similarity & Distance



"""### Loading a pre trained google news word2Vec"""

import gensim.downloader as api

model = api.load("fasttext-wiki-news-subwords-300")

print(model["king"])

model.most_similar("king")

model.most_similar(positive=['king', 'woman'], negative=['man'])

model.doesnt_match(["apple", "banana", "car", "mango"])



"""### Training Own model"""

sentences = [
    ["machine", "learning", "is", "amazing"],
    ["deep", "learning", "is", "a", "subset", "of", "machine", "learning"],
    ["artificial", "intelligence", "is", "the", "future"],
    ["word2vec", "converts", "words", "into", "vectors"]
]

from gensim.models import Word2Vec
model2 = Word2Vec(sentences, vector_size=50, window=5, min_count=1, workers=4)

model2.wv["machine"]

model2.wv.most_similar("learning")

from scipy.spatial.distance import cosine
from numpy.linalg import norm

# Distance
norm(model["man"] - model["woman"])

# Cos Similarity
cosine(model["kingdom"], model["people"])